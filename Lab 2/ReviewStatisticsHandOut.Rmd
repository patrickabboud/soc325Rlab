---
title: "Review of Statistical Inference"
author: "Zack W Almquist (University of Washington)"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,comment = "")

library(fivethirtyeight)
library(forcats)
library(knitr)
library(xtable)
options(digits=3)

perm_test<-function(outcome,group,size,pvar){
shuffle<-sapply(1:size,function(x){sample(x=group,size=length(group),replace=FALSE)})
label<-levels(group)
ng<-sapply(label,function(x){sum(x==group)})
bygroup<-lapply(1:length(label),function(y){apply(shuffle,2,
                function(x){sum(outcome[x==label[y]]==pvar)/ng[y]})})
## Assume just two groups
out<-bygroup[[1]]-bygroup[[2]]
obs<-abs(sum(outcome[group==label[1]]==pvar)/ng[1]-sum(outcome[group==label[2]]==pvar)/ng[2])
pval<-sum(out>=obs)/length(out)
list(sim=out,obs=obs,pval=pval)
}

```


## Preliminaries

**Definition** *Data*: Data (in this class) will be a vector or matrix of values. E.g.,

\begin{center}
```{r,messages=FALSE,warning=FALSE,results='asis',echo=FALSE}
exampMat<-head(USPersonalExpenditure,10)
print(xtable(exampMat),floating=FALSE)
```
\end{center}

\vspace{.1in}

**Definition** *Statistic*: A statistic $t(Y)$ is any function of the data. E.g.,

$$\bar{Y} = \frac{1}{n}\sum_{i=1}^n Y_i$$
**Definition** *Descriptive data analysis*: A representation of the main features of a dataset via a set of statistics $t_1(Y),\dots,t_k(Y)$. E.g., 

- Mean: $\bar{X}= \frac{1}{n}\sum_{i=1}^n X_i$
    + Proportion: $\hat{p} = \frac{Part}{Total} = \frac{1}{n}\sum_{i=1}^n 1\{X_i \in G\}$
- Median: Half the population is above and half below VALUE
- Mode: Most common VALUE
- Variance: $\hat{Var}(X) = \frac{1}{n-1}\sum{i=1}^n (X_i-\bar{X})^2$
- Standard Deviation: $\hat{SD}(X) = \sqrt{\hat{Var}(X) }$
- Standard Error: $SE(\bar{X}) = \sqrt{\hat{Var}(X)/n}$

\vspace{.1in}

**Definition** *Test statistic*: A *test statisti* is a standardized value that is calculated from sample data durning a hypothesis test.

\vspace{.1in}

**Definition** *Statistical Hypothesis test*: Evaluates two mutually exclusive statements about a population to determine which statement is best supported by the (sampled) data.

\vspace{.3in}

**Definition** *Distribution*: For this class distribution is the histogram or normalized frequency counts of our data. For example: Say we have the dataset of airlines by fatal accidents from 1985-199. (Below is the first 5 rows)

\begin{center}
```{r,messages=FALSE,warning=FALSE,results='asis',echo=FALSE}
data("airline_safety")
data<-airline_safety[,c("airline","fatal_accidents_85_99")]
exampMat<-head(data,5)
print(xtable(exampMat),floating=FALSE)
```
\end{center}

Then the distribution of fatal airline accidents from 1985-1999 would be the histogram of this data,

With the Frequency histogram being:
```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/4,fig.height=12/4}
hist(data$fatal_accidents_85_99,xlab="fatal airline accidents from 1985-1999",main="Frequency",probability =TRUE)
```

and the normalized (disribution) histogram being:


```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/4,fig.height=12/4}
hist(data$fatal_accidents_85_99,xlab="fatal airline accidents from 1985-1999",main="Distribution")
```


# What are we doing and why?

An introduction to statistical inference is really an introductuction to *statistical decision* making. To make a decision we need to be able to evaluate the *meaning* of our descriptive statistics. Typically this takes two majore forms: (1) comparison of two groups or (2) comparison against an assumed baseline.

## Hypothesis testing algorithem 

1. Question that can be answered with data. (E.g., is gender discrimination occuring in corperation the Tech sector?)
2. State **null** (usually 0 or status quo) and state **alternative** hypothesis (depends on question).
    + Make sure and also write down the desriptive statistic or comparison statistics of interest.
3. Perform a *statistical test*.
4. Evaluate evidence.

### Writing a Null versus Alternative Hypothesis

- Language for null hypothesis:
    + $H_0$: The null hypothesis of BLANK is that BLANK is VALUE or 0.
- Language for the alternative hypothesis:
    + $H_A$: The alternative hypothesis is that BLANK is $>$ VALUE, $<$ VALUE or $\neq$ VALUE.

BLANK is your statistics (e.g., mean value of X).

### Test statistic and evaluation of the Hypothesis

So far in class we have two *Null* hypothesis we can ask:

- $H_0$: Label BLANK does not matter.
- $H_0$: The STATISTIC of BLANK is VALUE or 0.

with 

- $H_A$: Label BLANK is related to OUTCOME BLANK.
- $H_A$: The STATISTIC of BLANK is $>$ VALUE, $<$ VALUE or $\neq$ VALUE.

#### Simulation/Permutation Test for Labeled data

**Example 1**

**The Problem** (Taken From [Fivethirtyeight](http://www.fivethirtyeight.com/)). Does education level affect wether one cares about the oxford comma?

```{r,echo=FALSE,warning=FALSE,message=FALSE}
data(comma_survey)
data<-comma_survey[,c("education","care_data")]
data$education<-fct_recode(data$education, 
                             "Associate degree or less" = "Less than high school degree",
                             "Associate degree or less" = "High school degree",
                             "Associate degree or less" =  "Some college or Associate degree",                        
                             "Bachelor degree or higher"=  "Bachelor degree",
                             "Bachelor degree or higher"=   "Graduate degree")
data$care_data<-fct_recode(data$care_data,
                           "No" = "Not at all",
                           "No" = "Not much",
                           "Yes" = "Some",
                           "Yes" = "A lot")
data<-data[!is.na(data[,1]),]
kable(addmargins(table(data)))
pTest<-perm_test(data$care_data,data$education,10,"Yes")
```

Hypothesis (one tail):

- $H_0$: Label EDUCATION does not matter. ($p_{BDH}-p_{ADL}=0$)
- $H_A$: Label EDUCATION matters and we expect $p_{BDH}>p_{ADL}$.
- $H_A$: Label EDUCATION matters and we expect $p_{BDH}<p_{ADL}$.


Test Statistic is:

$$\hat{p}_{BDH}-\hat{p}_{ADL} = \frac{294}{620} - \frac{158}{406} = `r pTest$obs`$$

**Simulation/Permutation Procedure**

- We assume the (NULL) labels have no meaning, so if we randomly re-assign the label to the outcome data we can acquire our NULL distribution.
    + If the observed test statistics is in the center of this distribution then we have strong evidence of the NULL hypothesis
    + If the observed test statistics is in the tails then we have evidence of the alternative hypothesis.


**Simulated Test (10 Simulations)**

```{r,echo=FALSE,warning=FALSE,message=FALSE}
#kable(data.frame(sim=pTest$sim))
round(pTest$sim,3)

```

**Resulting plot (1000 simulations):**

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/3,fig.height=12/3}
save<-perm_test(data$care_data,data$education,1000,"Yes")
plot(table(round(save$sim,2))/sum(table(save$sim)), 
     type="h",xlab="",ylab="",lwd = 5,col=rgb(0,0,0,1))
lines(table(round(save$sim[save$sim > save$obs],2))/sum(table(save$sim)), 
      type="h",xlab="",ylab="",lwd = 5,col=rgb(0,0,1,1))
abline(v=save$obs,col="red")
```

**Evaluation**

- $H_A$: $p_{BDH}>p_{ADL}$

```{r,echo=FALSE,warning=FALSE,message=FALSE,results='asis'}
simTest<-paste(round(pTest$sim,3),rep(" > ",length(pTest$sim)),rep(round(pTest$obs,3),length(pTest$sim)), rep(" = ",length(pTest$sim)),as.numeric(pTest$obs<pTest$sim),sep="")
kable(data.frame("Sim Test"=simTest))
```

**Evaluation** We reject the null. We have strong evidence that $H_A$ is true as the observed value is always higher than the randomly labeled simulation.


- $H_A$: $p_{BDH}<p_{ADL}$

```{r,echo=FALSE,warning=FALSE,message=FALSE,results='asis'}
simTest<-paste(round(pTest$sim,3),rep(" < ",length(pTest$sim)),rep(round(pTest$obs,3),length(pTest$sim)), rep(" = ",length(pTest$sim)),as.numeric(pTest$obs>pTest$sim),sep="")
kable(data.frame("Sim Test"=simTest))
```

**Evaluation** We accept the null. We have no evidence that $H_A$ is true as the observed value is always higher than the randomly labeled simulation.

Hypothesis (two tail):

- $H_A$: Label EDUCATION matters and we expect $p_{BDH} \neq p_{ADL}$.

```{r,echo=FALSE,warning=FALSE,message=FALSE,results='asis'}
simTest<-paste(round(pTest$sim,3),rep(" > ",length(pTest$sim)),rep(
  paste("|",round(pTest$obs,3),"|"),length(pTest$sim)), rep(" = ",length(pTest$sim)),as.numeric(abs(pTest$obs)<pTest$sim),sep="")
kable(data.frame("Sim Test"=simTest))
```

Where $|\cdot|$ is the absolute value function (i.e., $|-5|=5$).

**Evaluation** 

We have *strong* evidence  that $p_{BDH}\neq p_{ADL}$ is true as the absolute value of observed value is always higher than the randomly labeled simulation.

## Statistical Decision Making

Now say we want to formalize our **evaluation** of our hypothesis and provide a since of how confident we are in our conclusion.


### Statistical Significance ($\alpha$-level and P-Value)

We say that we have *Statistica Significance* (i.e., we have evidence our $H_A$ is correct) if our **P-Value** is less than some $\alpha$, and that the test is **NOT Statistically Significant** if the **P-Value** is greater than $\alpha$.


### Calculating P-Value (Simulation/Permutation Test)

To calculate the **P-Value** of our permutation test we simply sum uo the number of times our **Test Statistic** is greater than the *simulated* values. 

**Example (One-tail)**

$H_A$: $p_{BDH}>p_{ADL}$

```{r,echo=FALSE,warning=FALSE,message=FALSE,results='asis'}
simTest<-paste(round(pTest$sim,3),rep(" > ",length(pTest$sim)),rep(round(pTest$obs,3),length(pTest$sim)), rep(" = ",length(pTest$sim)),as.numeric(pTest$obs<pTest$sim),sep="")
kable(data.frame("Sim Test"=simTest))
```

$$P-Value = \frac{`r paste(rep(0,10),collapse="+")`}{10} = 0$$

*Conclusion*: Statistically Significant

**Example (Two-tail)**

$H_A$: $p_{BDH} \neq p_{ADL}$.

```{r,echo=FALSE,warning=FALSE,message=FALSE,results='asis'}
simTest<-paste(round(pTest$sim,3),rep(" > ",length(pTest$sim)),rep(
  paste("|",round(pTest$obs,3),"|"),length(pTest$sim)), rep(" = ",length(pTest$sim)),as.numeric(abs(pTest$obs)<pTest$sim),sep="")
kable(data.frame("Sim Test"=simTest))
```


$$P-Value_{\textrm{one tail}} = \frac{`r paste(rep(0,10),collapse="+")`}{10} = 0$$

$$P-Value_{\textrm{two tail}} = 2 \times P-Value_{\textrm{one tail}}  =  0$$

*Conclusion*: Statistically Significant

### Selecting $\alpha$-level

Traditionally $\alpha=0.05$. If $\alpha$ is \underline{NOT SPECIFIED} then $\alpha=0.05$. Otherwise $\alpha$ can vary -- we will cover why and when we cover **Type 1** and **Type 2** errors.

**Example (One-tail)**

$H_A$: $p_{BDH}>p_{ADL}$

- $P-Value_{\textrm{one tail}} = 0 < \alpha=0.05$. *Statistically Significant*

**Example (Two-tail)**

$H_A$: $p_{BDH} \neq p_{ADL}$.

- $P-Value_{\textrm{two tail}} = 0 < \alpha=0.05$. *Statistically Significant*


# Normal Approximation

\begin{center}
What if we want to test more complex hypothesis than does LABEL matter? 
\end{center}

- We cand do this! We will use the Normal Distribution to evaluate our descriptive statistics
- Second Question: What is the Normal Distribution?

## The Normal Distribution

The Normal Distribution (or Gaussian Distribution) is the classic *unimodal*, *symmetric* distribution that typically looks as follows:

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/3,fig.height=12/3}
curve(dnorm,-5,5)
```

- The Normal Distribution is defined by its mean ($\mu$) and its standard diviation (SD) ($\sigma$)  [or by its variance $\sigma^2$]
- Short hand $N(\mu,\sigma)$


### Properties of Normal Distributions

- We can Transform normal distributions and perserve key identities (i.e., if $x_1<x_2$ and we transform the data $x_1^* <x_2^*$, etc)
- What does this mean in practice? 
    + If we transform out observed mean ($X$) into a known distribution (e.g., $Z$) we can readily perform our hypothesis tests!

## Exampe: SAT and ACT Scores

- SAT and ACT are by definition Normaly Distributed (ETS and ACT make sure this is true)

\begin{center}

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/3,fig.height=12/3,results='asis'}
mat<-matrix(NA,nc=2,nr=2)
mat[1,1]<-1500
mat[1,2]<-21
mat[2,1]<-300
mat[2,2]<-5
colnames(mat)<-c("SAT","ACT")
rownames(mat)<-c("Mean","SD")
print(xtable(mat),floating = FALSE)
```

\end{center}

We can plot our SAT Normal Distribution

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/3,fig.height=12/3}
curve(dnorm(x,mean=1500, sd = 300),600,2400,xlab="SAT",ylab="Distribution",main="N(1500,300)")
```

And we can plot our ACT Normal Distribution

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/3,fig.height=12/3}
curve(dnorm(x,mean=21, sd = 5),5,35,xlab="ACT",ylab="Distribution", main="N(21,5)")
```

## Z-Distribution (Comparing two normals)

What if we want to compare two people, Bill who has a SAT Score of 1300 and June that has an ACT score of 25? We can't do it directly! But we can transform both into a Normal Distribution that has the same mean and standard deviation (allowing us to compare them!).

### Z-Distribution ($N(0,1)$)

The $Z$ distribution is a very spetial case of the Normal distribution, it is the case where the mean equals zero and the standard deviation equals 1. I.e., $N(0,1)$

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/3,fig.height=12/3}
curve(dnorm,-5,5)
```

### Z-score

The $Z$-Score is the equation for transforming normaly distributed data into the $N(0,1)$ case. Given $X$ from a normal distribution $N(\mu,\sigma)$ we can subtract the mean to center it at 0:

$$X-\mu$$
We can then dived it by $\sigma$ to normalize the standard deviation to 1 (i.e., $\sigma/\sigma=1$):
$$\frac{X-\mu}{\sigma}$$
We call this normalized value a $Z$-score and represent it as,
$Z= \frac{X-\mu}{\sigma}$.

Some important notes, Say we have $X_1$ from a $N(\mu_1,\sigma_1)$ and  $X_2$ from a $N(\mu_2,\sigma_2)$ and we transform both into the Z-score:
$$Z_1 = \frac{X_1-\mu_1}{\sigma_1} \textrm{ and } Z_2 = \frac{X_2-\mu_2}{\sigma_2}$$
Then if $Z_1 > Z_2$ then we can say $X_1$ is of higher rank than $X_2$. Let's do an example with our SAT/ACT case.

**ACT/SAT Example**

$$Z_{Bill}=\frac{1300-1500}{300} = -0.67$$
and

$$Z_{June} = \frac{25-21}{5} = 0.8$$
Thus we know know that June scored higher Bill! 

- What if we want to know their relationship to the general population of interests? I.e., what is their percentile rank?

### Percentile Rank (Cummulative Distribution Function)

To find the Percentile Rank of Z-Score we need to find the area under the curve upto our Z-score (this is the Cummulative Distribution because we are summing all of the area upto the Z-score value), i.e.,

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/3,fig.height=12/3}
X <- seq(-4,4,0.01)
Y <- dnorm(X)
plot(X, Y, type='l', axes=F, xlim=c(-3.4,3.4))
axis(1, at=-3:3, label=(1500+300*(-3:3)), cex.axis=0.7)
these <- which(X <= -0.67)
polygon(c(X[these[1]], X[these],X[rev(these)[1]]), c(0,Y[these],0), col="grey")
```

and

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/3,fig.height=12/3}
X <- seq(-4,4,0.01)
Y <- dnorm(X)
plot(X, Y, type='l', axes=F, xlim=c(-3.4,3.4))
axis(1, at=-3:3, label=(21+5*(-3:3)), cex.axis=0.7)
these <- which(X <= 0.8)
polygon(c(X[these[1]], X[these],X[rev(these)[1]]), c(0,Y[these],0), col="grey")
```

This area can be calulated (With R) as,

$$P(X < Z = z ) = pnorm(Z_{value})$$
I.e., June has percentile rank of $`pnorm(0.8)`=`r pnorm(0.8)`$ and Bill has percentile rank of $`pnorm(-0.67)`=`r pnorm(-0.67)`$.

## Finding Score based on Percentile Rank

What if Bill decided he did not like how he performed and that we want to score in the top 90\% of score takers?

- Step 1: Calculate the needed Z-Score, $Z^{*}$=qnorm(.9) = `r qnorm(.9)`
- Step 2: Sove for X!
$$Z = \frac{X-\mu}{\sigma}$$
$$ X = Z\sigma+\mu$$
$$X = `r qnorm(.9)`*300+1500 = `r qnorm(.9)*300+1500`$$

What if Bill had wanted to score in the top 2.5\% of score takers? This would have been Z=`r qnorm(.975)`!

**More Percentile Examples**

What if I run a school and I plan to accept anyone in 40\% to 85\% ACT score how would I figure out the range?

We can use this to find $P(Z_L^{*} < Z < Z_U^{*} )$
    + First we need to find $P(Z<Z_L^{*}) = 0.4$ to do this we can again use `qnorm(.4)` = `r qnorm(.4)` to find $Z_L^{*}$,
    $$X_L = Z_L^{*}\sigma+\mu = `r qnorm(.4)`*5+300 =  `r qnorm(.4)*5+300`$$

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/3,fig.height=12/3}
X <- seq(-4,4,0.01)
Y <- dnorm(X)
plot(X, Y, type='l', axes=F, xlim=c(-3.4,3.4))
axis(1, at=-3:3, label=(21+5*(-3:3)), cex.axis=0.7)
these <- which(X <= -0.25 )
polygon(c(X[these[1]], X[these],X[rev(these)[1]]), c(0,Y[these],0), col="grey")
```

    + Next I need to find $Z_U^*$ such that $P(Z<Z_U^*)$, which is `r qnorm(.85)`

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/3,fig.height=12/3}
X <- seq(-4,4,0.01)
Y <- dnorm(X)
plot(X, Y, type='l', axes=F, xlim=c(-3.4,3.4))
axis(1, at=-3:3, label=(21+5*(-3:3)), cex.axis=0.7)
these <- which(X > 1.04 )
polygon(c(X[these[1]], X[these],X[rev(these)[1]]), c(0,Y[these],0), col="grey")
```

  $$X_U = Z_U^{*}\sigma+\mu = `r qnorm(.85)`*5+300 =  `r qnorm(.85)*5+300`$$

So the school will except any one from `r qnorm(.4)*5+300` to `r qnorm(.85)*5+300`.

## Add, Subtracting Normal Distributions

Remember, $P(Z) =1$, i.e., the total sum of all the area under the curve is 1.

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/3,fig.height=12/3}
X <- seq(-4,4,0.01)
Y <- dnorm(X)
plot(X, Y, type='l', axes=F, xlim=c(-3.4,3.4))
axis(1, at=-3:3, label=(21+5*(-3:3)), cex.axis=0.7)
these <- which(X <= 21+5*3)
polygon(c(X[these[1]], X[these],X[rev(these)[1]]), c(0,Y[these],0), col="grey")
```

and $pnorm(Z^{*})$ is the sum of all probability (area) up to $Z^{*}$, e.g. pnorm(1.2) = `r pnorm(1.2)`


```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/3,fig.height=12/3}
X <- seq(-4,4,0.01)
Y <- dnorm(X)
plot(X, Y, type='l', axes=F, xlim=c(-3.4,3.4))
axis(1, at=-3:3, label=(21+5*(-3:3)), cex.axis=0.7)
these <- which(X <=  1.2 )
polygon(c(X[these[1]], X[these],X[rev(these)[1]]), c(0,Y[these],0), col="grey")
```

## Z-Test, Statistical Decision making with the Normal Distribution

### Hypothesis Test with Z-Test

**Define Your Hypothesis**

- $H_0$ X is equal to VALUE.

\vspace{.1in}

- $H_A$ X is less than VALUE (one sided test).
- $H_A$ X is greater than VALUE (one sided test).
- $H_A$ X is not equal to VALUE (two sided test).

**Define Your Test**

- We say $H_0$ is true when $X< Z_{Critival}$ and we say we reject $H_0$ and accept $H_A$ when $X > Z_{Critival}$. 
- We find  $Z_{Critical}$ based on our chosen $\alpha$-level (if not specified $\alpha=0.05$).
    + Thus $Z_{Critical}$ is really $Z_{\alpha}$
    + A *Critical Value* for a two sided Z-test for $\alpha=0.05$ is 1.96
    + A *Critical Value* for a one sided Z-test for $\alpha=0.05$ is 1.65 (upper taile) or -1.65 (lower tail)

The Z-test is as follows,

Find

$$Z = \frac{X-VALUE}{\sigma}$$

Compare with $Z_{\alpha}$. 

Accept or reject Null Hypothesis.

### Z-Test Example

From [fivethirtyeight.com](http://www.fivethirtyeight.com) we the R package (`fivethirtyeight`) we can get the party affiliation and age of member of congress, between January 1947 and Februrary 2014. Below is the first five entries of the data set.

```{r,echo=FALSE,warning=FALSE,message=FALSE}
data("congress_age")
data<-congress_age[,c("party","age")]
kable(head(data,5))
```

From this data we learn that age for each party is normally distributed with Democrats being N(`r mean(data$age[data$party=="D"])`, `r sd(data$age[data$party=="D"])`) and Rublicans being N(`r mean(data$age[data$party=="R"])`, `r sd(data$age[data$party=="R"])`).

**Question**
Is Edward James Patten elected in 1979 at 73.4 years young for his party (Democrats)

$H_0$: Edward's age of 73.4 is equal to average of `r mean(data$age[data$party=="D"])`.

$H_A$: Edward's age of 73.4 is less/greater/not equal to the `r mean(data$age[data$party=="D"])`.

$$Z_{value} = \frac{73.4-53.4}{11} = 1.8$$
H_A: (Lower tail) Edward's age of 73.4 is less than 53.4

1.8 is greater than $qnorm(.05)$ so we reject $H_A$ and accept the NUll.


H_A: (upper tail) Edward's age of 73.4 is greater than 53.4

1.8 is greater than $qnorm(.95)$ so we accept $H_A$ and reject $H_0$.

H_A: (two tail) Edward's age of 73.4 is not equal 53.4.

$|Z_{value}=1.8$ is less than $qnorm(.975)$ (two tail, so 2.5\% in each tail) so we reject $H_A$ and accept $H_0$. **Notice that two-tail test is most stringent -- Default to two-tail test if not specified**

## Statistical Significance and P-Value

P-Vaue = $P(|Z_{Value}|<Z)$, so for the last example $P(|1.8|<Z) = 2*(1-pnorm(abs(1.8)))= `r 2*(1-pnorm(abs(1.8)))`$.

We say the P-Value is *Statistically Significant* if P-Vaue $< \alpha$. So in the above example if $\alpha=0.05$ then we would say *Not Statistically Significant*, since `r 2*(1-pnorm(abs(1.8)))`; however if we took $\alpha=0.1$ then  `r 2*(1-pnorm(abs(1.8)))` is less than $\alpha$ and we say it is statistically significant.

## Type 1 and Type 2 Errors

**Definition** Type 1 error is False Postives (i.e., you predict 1 when you should have predicted 0).

**Definition** Type 2 error is False Negatives (i.e., you predict 0 when you should have predicted 1).

Your $\alpha$-level of significance controls your Type 1 error. I.e., your Type 1 error is lower the smaller your $\alpha$ level; however, the smaller your $\alpha$ level the larger your Type 2 errors (This is also known as the *Power of your test*). You have to balance these two issues out. We will do more with this later in the semester. 

- Key take away: lower $\alpha$ lower type 1 error and higher type 2 error. 
- When might this be acceptable: Example, medical studies. We want to know the medicine works, we want low type 1 errors!


# What if our data is not from a normal distribution?

We can do this! We will do this by taking advantage of the **Central Limit Theorem (CLT)** . CLT provides us an amazing mathematical observation: The distribution of *any* mean statistic (reguardless of data form/type) given large enough sample size is normally distributed (under most reasonable conditions).


## Example Mean of fatal airline accidents from 1985-1999

Let's return to our distribution of fatal airline accidents from 1985-1999:

\begin{center}
```{r,messages=FALSE,warning=FALSE,results='asis',echo=FALSE}
data("airline_safety")
data<-airline_safety[,c("airline","fatal_accidents_85_99")]
exampMat<-head(data,5)
print(xtable(exampMat),floating=FALSE)
```
\end{center}

Then the distribution of fatal accidents from 1985-1999 would be the histogram of this data,

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/4,fig.height=12/4}
hist(data$fatal_accidents_85_99,xlab="fatal accidents from 1985-199",main="")
```

**This distribution is clearly not NORMAL!!**

What is the distribution of means? How can we understand that!?

- Let's think back on our simulation/permutation test. 
- We can extend the idea to what is known as the *bootstrap*
    + The *bootstrap* is simple/clever idead
    + Resample the data with replacement and recalulate the statistics of interest
    + Repeat K number of times

Let's do this procedure for the mean statistic for the fatal accidents from 1985-1999 data.

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/4,fig.height=12/4}
data("airline_safety")
data<-airline_safety[,c("airline","fatal_accidents_85_99")]
boot<-sapply(1:10000,function(x){mean(sample(data$fatal_accidents_85_99,length(data$fatal_accidents_85_99),replace = TRUE))
  })
hist(boot,xlab="Average fatal airline accidents from 1985-1999",main="Mean Distribution",probabilit=TRUE)
abline(v=mean(data$fatal_accidents_85_99),col="red")
```

**So what is the key observation?**
  - While the underlying data is clearly not Normal, the distribution of means is Normal!

## How do we use CLT for Statistical Decision Making?

## Z Transformation under CLT

Formally, $Z = \frac{t(x)-\mu}{SE}$. Under CLT the mean is $\bar{X}$, but $SE = \sigma/\sqrt{n}$. So our $Z_{value} =\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}$. I.e., $\bar{X}$ is distributed $N(\mu,\sigma/\sqrt{n})$.

## Hypothesis test under CLT

$H_0$: $\bar{X}$ is equal to VALUE.

$H_A$: $\bar{X}$ is less than/greater than/not equal to VALUE. (one sidede/one sided/ two sided)

**Example**

```{r,echo=FALSE,warning=FALSE,message=FALSE,fig.align='center',fig.wdith=8/4,fig.height=12/4}
data("airline_safety")
data<-airline_safety[,c("airline","fatal_accidents_85_99","fatal_accidents_00_14")]
```

We want to know if the average number of fatal accidents from 1985-1999 with mean `r mean(data$fatal_accidents_85_99)` is equal to the number of  fatal accidents from from 2000-2014 with `r mean(data$fatal_accidents_00_14)` and $\sigma = ` sd(unlist(data[,2:3]))`$ and $n= `r 2*NROW(data)`$.

**Hypothesis Test**

```{r, echo=FALSE,results='hide'}
se<-sd(unlist(data[,2:3]))/sqrt(2*NROW(data))
z<-(mean(data$fatal_accidents_85_99)- mean(data$fatal_accidents_00_14))/se
```

$H_0$: The average number of fatal accidents from 1985-1999 is equal to the number of  fatal accidents from from 2000-2014. ($\bar{X}=\bar{Y}$)

$H_A$:The average number of fatal accidents from 1985-1999 is not equal to the number of  fatal accidents from from 2000-2014. ($\bar{X}-\bar{Y} \neq 0$)

**Z-Value**

$SE = \frac{\sigma}{\sqrt{n}} = `r sd(unlist(data[,2:3]))/sqrt(2*NROW(data))`$

$$Z_{value} = \frac{`r mean(data$fatal_accidents_85_99)` - `r mean(data$fatal_accidents_00_14)`}{`r se`} = `r z`$$

Is `r z` greater than or less then `r qnorm(.975)`? 

*Conclusion* We accept $H_A$ and reject $H_0$.

## P-value and Statistical Significance

P-value is $\Pr(Z>Z_{value})$ = $\Pr(Z>Z_{value})$ = $\Pr(Z > `r z`) = 2*(1-pnorm(`r z`)) =  `r 2*(1-pnorm(z))`$ which is really tiny -- so we say this test is *Statistical Significant*.

## 95\% (1-$\alpha$) Confidence Intervals

Another important quality metric is the 95\% Confidence Interval (or 1-$\alpha$\% CI). The 95\% Confidence Interval is defined as follow,

$$ \bar{X} \pm Z_{1-\alpha/2} \times SE = \bar{X} \pm 1.96 \times SE$$
or 

$$\bar{X} -\bar{Y} \pm Z_{1-\alpha/2} \times SE$$
So in the last example the 95\% CI would be 

$$ `r mean(data$fatal_accidents_85_99)` - `r mean(data$fatal_accidents_00_14)` \pm `r pnorm(.975)`\times `r se`$$

Which is `r mean(data$fatal_accidents_85_99) - mean(data$fatal_accidents_00_14) - pnorm(.975)*se` to `r mean(data$fatal_accidents_85_99) -mean(data$fatal_accidents_00_14) + pnorm(.975)*se`

- Key take aways if your $(1-\alpha)$ CI does not cover zero then you know your P-Value is less than $\alpha/2$ and therefore your test is *Statistically Significant*
- Remember, **The 95\% CI guarntees that your population parameter is contained between your upper and lower values 95\% of the time.** NOT THAT YOUR POPULATION PARAMETER IS BOUNDED BY THE UPPER AND LOWER VALUES (this is non-intuitive, be careful!).

# Special Case of means, PROPORTIONS


Proportions are special case of mean that have very nice properties. 

- The CLT kicks at fairly low $n$ (i.e., $n>30$)
- The SE can be derived from the bernoulli distribution and is $SE=\sqrt{p(1-p)/n}$
- Two sample SE under standard NULL is $\hat{p} = \frac{A+B}{TOTAL}$ with $SE=\sqrt{\hat{p}(1-\hat{p})/n}$

### Example Congressional Representation and Age

From [fivethirtyeight.com](http://www.fivethirtyeight.com) and the R package (`fivethirtyeight`) we can get the party affiliation and age of member of congress, between January 1947 and Februrary 2014. Here we have printed out the first five rows.

```{r,echo=FALSE,warning=FALSE,message=FALSE}
data("congress_age")
data<-congress_age[,c("party","age")]
kable(head(data,5))
```

We are interested in the question "Are there more Democrats then Republicans under 40"? We have the following information:

- The number of Democrats under 40 is `r sum(data$age[data$party=="D"]<40)`.
- The number of Republicans under 40 is  `r sum(data$age[data$party=="R"]<40)`.
- The total number of Democrats is `r sum(data$party=="D")`.
- The total number of Republicans is `r sum(data$party=="R")`.



**Hypothesis Test**

$H_0$ The proportion of Democrats and Republicans under 40 is the same. $p_D-p_R = 0$

$H_A$ The proportion of Democrats and Repulicans under 40 is not the same. $p_D-p_R \neq 0$

**Z-value**

$$Z = \frac{\hat{p}_D-\hat{p}_R - 0}{\sqrt{\hat{p}(1-\hat{p})/n}}$$
```{r,echo=FALSE,results='hide'}
n<-sum(data$part%in%c("D","R"))
phat<-sum(data$age[data$part%in%c("D","R")]<40)/sum(data$part%in%c("D","R"))
se = sqrt(phat*(1-phat)/n)
pd<-sum(data$age[data$party=="D"]<40)/sum(data$party=="D")
pr<-sum(data$age[data$party=="R"]<40)/sum(data$party=="R")
```

$$ \hat{p}_D = \frac{`r sum(data$age[data$party=="D"]<40)`}{`r sum(data$party=="D")`} = `r sum(data$age[data$party=="D"]<40)/sum(data$party=="D")`$$
$$ \hat{p}_R = \frac{`r sum(data$age[data$party=="R"]<40)`}{`r sum(data$party=="R")`} = `r sum(data$age[data$party=="R"]<40)/sum(data$party=="R")`$$
$$\hat{p} = \frac{`r sum(data$age[data$party=="D"]<40)`+ `r sum(data$age[data$party=="R"]<40)`}{`r sum(data$party=="D")`+`r sum(data$party=="R")`} = `r sum(data$age[data$part%in%c("D","R")]<40)/sum(data$part%in%c("D","R"))`$$

$$SE = `r se`$$

Thus,

$$Z_{value} = \frac{`r pd`-`r pr`}{`r se`} = `r (pd-pr)/se`$$

So, $Z_{value}=`r (pd-pr)/se`$ is greater than 1.96 so we accept $H_A$ and reject $H_0$.

**Pvalue**
  
The P-value is $\Pr(Z>`r (pd-pr)/se`) = 2*(1-pnorm(`r (pd-pr)/se`)) = `r  2*(1-pnorm((pd-pr)/se))`$. So again we say that the difference between the proportion of Democrats and Republicans under 40 is statistically significant at $\alpha=0.05$ level.
  
  **95\% CI**
    
    
$$\hat{p}_D-\hat{p}_R \pm 1.96 \times \sqrt{\hat{p}(1-\hat{p})/n}$$
    
$$`r (pd-pr)` \pm 1.96 \times `r se`$$

Which tells us that upper and lower bounds are `r (pd-pr)-1.96*se` and `r (pd-pr)+1.96*se`. Which gives us a 95\% chance that the true population proportion is contained within this interval.




